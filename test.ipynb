{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: 39, Responses: 39\n",
      "\n",
      "Sample from DataFrame:\n",
      "                              input  \\\n",
      "24            fale sobre a maniçoba   \n",
      "26      quando é o círio de nazaré?   \n",
      "12   lugares para conhecer em belém   \n",
      "30             onde ficar em belém?   \n",
      "17  quais são os folclores do pará?   \n",
      "\n",
      "                                             response  \n",
      "24  A maniçoba é considerada a 'feijoada paraense'...  \n",
      "26  O Círio de Nazaré acontece no 2º domingo de ou...  \n",
      "12  Roteiro essencial: Dia 1 - Centro Histórico; D...  \n",
      "30  Em Belém, recomendo ficar no Centro Histórico,...  \n",
      "17  O folclore paraense é riquíssimo! Destacam-se:...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a balanced dataset with matching input-response pairs\n",
    "data = {\n",
    "    \"input\": [\n",
    "        \"oi\", \n",
    "        \"olá\", \n",
    "        \"bom dia\", \n",
    "        \"boa tarde\", \n",
    "        \"boa noite\",\n",
    "        \"conte-me sobre pará\", \n",
    "        \"fale sobre o estado do pará\", \n",
    "        \"qual a história do pará?\", \n",
    "        \"como é a cultura do pará?\",\n",
    "        \"pontos turísticos em belém\", \n",
    "        \"o que visitar em belém\",\n",
    "        \"atrações turísticas de belém\", \n",
    "        \"lugares para conhecer em belém\",\n",
    "        \"conte sobre o ver-o-peso\", \n",
    "        \"o que é o teatro da paz\", \n",
    "        \"fale sobre o mangal das garças\", \n",
    "        \"conte sobre a estação das docas\",\n",
    "        \"quais são os folclores do pará?\", \n",
    "        \"o que é o carimbó?\", \n",
    "        \"quem é o curupira?\", \n",
    "        \"o que significa boi-bumbá?\",\n",
    "        \"quais comidas típicas do pará?\", \n",
    "        \"o que é pato no tucupi?\", \n",
    "        \"conte sobre o tacacá\", \n",
    "        \"fale sobre a maniçoba\",\n",
    "        \"quais eventos importantes no pará?\", \n",
    "        \"quando é o círio de nazaré?\", \n",
    "        \"fale sobre o festival de carimbó\",\n",
    "        \"como chegar em belém?\", \n",
    "        \"qual o aeroporto de belém?\", \n",
    "        \"onde ficar em belém?\", \n",
    "        \"quando visitar belém?\",\n",
    "        \"história de belém\", \n",
    "        \"fale sobre o ciclo da borracha\", \n",
    "        \"conte sobre a cabanagem\",\n",
    "        \"sobre a amazônia no pará\", \n",
    "        \"quais rios importantes no pará?\", \n",
    "        \"quais animais vivem na floresta?\", \n",
    "        \"como é o clima no pará?\"\n",
    "    ],\n",
    "    \"response\": [\n",
    "        \"Olá! Como posso ajudar você com informações sobre Belém e o Pará?\",\n",
    "        \"Oi! Tudo bem? Estou aqui para falar sobre a cultura e turismo do Pará!\",\n",
    "        \"Bom dia! Belém é uma cidade cheia de cultura e história. Sobre o que gostaria de saber?\",\n",
    "        \"Boa tarde! Vamos conversar sobre as maravilhas do Pará?\",\n",
    "        \"Boa noite! O Pará tem muitas histórias para contar. Qual delas você quer ouvir?\",\n",
    "        \"O Pará é o segundo maior estado do Brasil, localizado na região Norte. Sua capital Belém é conhecida como a 'Metrópole da Amazônia'!\",\n",
    "        \"O estado do Pará é fascinante! Com área de 1,2 milhão de km², tem clima equatorial, vastos rios e biodiversidade incrível.\",\n",
    "        \"O Pará foi fundado em 1616 com a construção do Forte do Presépio (hoje Forte do Castelo). Teve períodos de riqueza no ciclo da borracha.\",\n",
    "        \"A cultura do Pará é uma mistura de tradições indígenas, africanas e portuguesas, com destaque para o carimbó, lendas amazônicas e o Círio de Nazaré.\",\n",
    "        \"Belém oferece incríveis atrações: Ver-o-Peso, Teatro da Paz, Mangal das Garças, Estação das Docas e Museu Emílio Goeldi.\",\n",
    "        \"Em Belém você deve visitar: Ver-o-Peso, Mangal das Garças, Estação das Docas, Teatro da Paz e Museu Emílio Goeldi.\",\n",
    "        \"Principais atrações: 1) Ver-o-Peso 2) Mangal das Garças 3) Estação das Docas 4) Teatro da Paz 5) Museu Goeldi.\",\n",
    "        \"Roteiro essencial: Dia 1 - Centro Histórico; Dia 2 - Ver-o-Peso; Dia 3 - Museu Goeldi; Dia 4 - Estação das Docas.\",\n",
    "        \"O Ver-o-Peso é o mercado mais famoso de Belém, fundado em 1625. É um complexo que inclui mercado de peixe, carne e Feira do Açaí.\",\n",
    "        \"O Teatro da Paz é um dos mais belos do Brasil, construído em 1878 durante o ciclo da borracha, em estilo neoclássico com elementos amazônicos.\",\n",
    "        \"O Mangal das Garças é um parque ecológico de 40.000 m² às margens do rio Guamá, com viveiro de borboletas e muitos pássaros.\",\n",
    "        \"A Estação das Docas é um complexo cultural no antigo porto de Belém, revitalizado em 2000, com restaurantes e espaço para eventos.\",\n",
    "        \"O folclore paraense é riquíssimo! Destacam-se: Carimbó, Boi-Bumbá, lendas como Curupira e Iara, Marujada e Círio de Nazaré.\",\n",
    "        \"O carimbó é uma dança tradicional paraense de origem afro-indígena, reconhecida como patrimônio cultural brasileiro.\",\n",
    "        \"O Curupira é uma lenda amazônica sobre um anão de cabelos vermelhos e pés virados que protege a floresta.\",\n",
    "        \"O Boi-Bumbá é uma festa folclórica que conta a história de Pai Francisco e Mãe Catirina com a morte e ressurreição de um boi.\",\n",
    "        \"A culinária paraense é uma explosão de sabores! Pratos imperdíveis: Pato no Tucupi, Tacacá, Maniçoba, Açaí e Vatapá de camarão.\",\n",
    "        \"O Pato no Tucupi é o prato mais emblemático do Pará. Pato assado cozido no tucupi (líquido da mandioca brava) com jambu.\",\n",
    "        \"O tacacá é uma sopa tradicional de rua com tucupi, goma, camarão seco e jambu, que causa formigamento na língua.\",\n",
    "        \"A maniçoba é considerada a 'feijoada paraense', feita com folhas de maniva cozidas por 7 dias com carnes.\",\n",
    "        \"Principais eventos: 1) Círio de Nazaré (outubro) 2) Arraial do Pavulagem (junho) 3) Festival de Carimbó (agosto) 4) Marujada (dezembro).\",\n",
    "        \"O Círio de Nazaré acontece no 2º domingo de outubro em Belém, reunindo 2 milhões de pessoas na maior procissão católica do Brasil.\",\n",
    "        \"O Festival de Carimbó ocorre em agosto, mês do aniversário de Mestre Verequete, com apresentações do ritmo tradicional.\",\n",
    "        \"Para chegar em Belém: 1) Aéreo - Aeroporto Internacional Val-de-Cans 2) Rodoviário - Terminal Rodoviário 3) Fluvial - Barcos.\",\n",
    "        \"O Aeroporto Internacional de Belém/Júlio Cezar Ribeiro (código BEL) fica a 10 km do centro, recebendo voos de todo Brasil.\",\n",
    "        \"Em Belém, recomendo ficar no Centro Histórico, Batista Campos ou Nazaré. Reserve com antecedência no período do Círio.\",\n",
    "        \"A melhor época para visitar é de junho a novembro (menos chuvas). Dezembro a maio é mais chuvoso, mas com temperaturas agradáveis.\",\n",
    "        \"Belém foi fundada em 12/01/1616 com a construção do Forte do Presépio (hoje Forte do Castelo) para defender a região.\",\n",
    "        \"O ciclo da borracha (1879-1912) trouxe riqueza a Belém, que foi modernizada com teatros e palacetes, sendo chamada 'Paris n'América'.\",\n",
    "        \"A Cabanagem (1835-1840) foi revolta popular no Grão-Pará contra pobreza, com cerca de 30 mil mortos. Símbolo da resistência amazônica.\",\n",
    "        \"O Pará abriga parte da maior floresta tropical - a Amazônia, com biodiversidade única como onça-pintada, arara-azul e boto-cor-de-rosa.\",\n",
    "        \"Rios importantes: 1) Amazonas 2) Tocantins 3) Xingu 4) Tapajós 5) Jari. A hidrovia é vital para transporte na região.\",\n",
    "        \"A fauna inclui onças, antas, macacos, botos, araras e tucanos. No Museu Goeldi você vê muitos destes animais.\",\n",
    "        \"O clima em Belém é quente e úmido (média 26°C), com chuvas intensas de dezembro a maio. De junho a novembro é menos chuvoso.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Verify lengths match\n",
    "print(f\"Inputs: {len(data['input'])}, Responses: {len(data['response'])}\")  # Should be equal\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV for future use\n",
    "df.to_csv('para_tourism_dataset.csv', index=False)\n",
    "\n",
    "print(\"\\nSample from DataFrame:\")\n",
    "print(df.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 03:25:22.664656: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743488722.707073  696276 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743488722.719600  696276 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743488722.811009  696276 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743488722.811027  696276 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743488722.811028  696276 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743488722.811029  696276 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-01 03:25:22.822162: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization, Bidirectional, RepeatVector\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Sample dataset (in a real scenario, you'd have a much larger dataset)\n",
    "# data = {\n",
    "#     \"input\": [\n",
    "#         \"oi\", \"olá\", \"bom dia\", \"boa tarde\", \"boa noite\", \n",
    "#         \"conte-me sobre pará\", \"fale sobre a amazônia\", \n",
    "#         \"quais são os folclores da região?\", \"o que é o carimbó?\",\n",
    "#         \"quem é o curupira?\", \"o que significa boi-bumbá?\",\n",
    "#         \"quais animais vivem na floresta?\", \"como é o clima no pará?\",\n",
    "#         \"quais frutas típicas da amazônia?\", \"o que é lendário na região?\"\n",
    "#     ],\n",
    "#     \"response\": [\n",
    "#         \"olá! como posso ajudar?\", \"oi! tudo bem?\", \"bom dia! em que posso ajudar?\", \n",
    "#         \"boa tarde! pergunte sobre o pará e a amazônia!\", \"boa noite! vamos conversar sobre o folclore amazônico?\",\n",
    "#         \"o pará é um estado incrível no norte do brasil, cheio de cultura e natureza!\", \n",
    "#         \"a amazônia é a maior floresta tropical do mundo, com biodiversidade única!\",\n",
    "#         \"a região tem folclores ricos como curupira, boi-bumbá, vitória-régia e mais!\",\n",
    "#         \"carimbó é uma dança tradicional paraense com influências indígenas, africanas e europeias!\",\n",
    "#         \"curupira é um protetor das florestas com pés virados para trás que confunde caçadores!\",\n",
    "#         \"boi-bumbá é uma festa folclórica que conta a história de um boi que ressuscita!\",\n",
    "#         \"onças, araras, botos, sucuris e milhões de espécies vivem na amazônia!\",\n",
    "#         \"o pará tem clima equatorial, quente e úmido o ano todo com muitas chuvas!\",\n",
    "#         \"açaí, cupuaçu, bacuri, taperebá e pupunha são frutas deliciosas da região!\",\n",
    "#         \"a região é cheia de lendas como iara, boto cor-de-rosa e mapinguari!\"\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions\n",
    "def preprocess_text(text):\n",
    "    # Normalize and clean text\n",
    "    text = unicodedata.normalize('NFKD', text.lower()).encode('ASCII', 'ignore').decode('ASCII')\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing\n",
    "df['input_clean'] = df['input'].apply(preprocess_text)\n",
    "df['response_clean'] = df['response'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e': 1, 'o': 2, 'de': 3, 'a': 4, 'para': 5, 'do': 6, 'belem': 7, 'com': 8, 'em': 9, 'sobre': 10, 'no': 11, 'da': 12, 'que': 13, 'das': 14, 'como': 15, 'carimbo': 16, 'uma': 17, 'dia': 18, 'veropeso': 19, 'estacao': 20, 'docas': 21, 'cirio': 22, 'nazare': 23, '2': 24, 'um': 25, 'fale': 26, 'teatro': 27, 'paz': 28, 'mangal': 29, 'garcas': 30, 'quais': 31, 'tucupi': 32, 'na': 33, 'museu': 34, 'goeldi': 35, '1': 36, '3': 37, 'boa': 38, 'historia': 39, 'cultura': 40, 'visitar': 41, 'conte': 42, 'pato': 43, 'ciclo': 44, 'borracha': 45, 'voce': 46, 'brasil': 47, 'foi': 48, 'forte': 49, '4': 50, 'mais': 51, 'paraense': 52, 'estado': 53, 'qual': 54, 'atracoes': 55, 'curupira': 56, 'boibumba': 57, 'tacaca': 58, 'manicoba': 59, 'eventos': 60, 'importantes': 61, 'festival': 62, 'aeroporto': 63, 'amazonia': 64, 'rios': 65, 'floresta': 66, 'clima': 67, 'maior': 68, 'regiao': 69, 'centro': 70, 'tradicional': 71, 'junho': 72, 'dezembro': 73, 'oi': 74, 'ola': 75, 'bom': 76, 'tarde': 77, 'noite': 78, 'quando': 79, 'chegar': 80, 'ficar': 81, 'cabanagem': 82, 'animais': 83, 'as': 84, 'tem': 85, 'biodiversidade': 86, 'fundado': 87, 'construcao': 88, 'presepio': 89, 'hoje': 90, 'castelo': 91, 'riqueza': 92, 'lendas': 93, 'emilio': 94, 'principais': 95, '5': 96, 'historico': 97, 'mercado': 98, 'complexo': 99, 'inclui': 100, 'acai': 101, 'muitos': 102, 'cultural': 103, 'marujada': 104, 'amazonica': 105, 'camarao': 106, 'jambu': 107, 'outubro': 108, 'agosto': 109, 'internacional': 110, 'rodoviario': 111, 'novembro': 112, 'menos': 113, 'chuvas': 114, 'maio': 115, 'chuvoso': 116, 'conteme': 117, 'pontos': 118, 'turisticos': 119, 'turisticas': 120, 'lugares': 121, 'conhecer': 122, 'sao': 123, 'os': 124, 'folclores': 125, 'quem': 126, 'significa': 127, 'comidas': 128, 'tipicas': 129, 'onde': 130, 'vivem': 131, 'posso': 132, 'ajudar': 133, 'informacoes': 134, 'tudo': 135, 'bem': 136, 'estou': 137, 'aqui': 138, 'falar': 139, 'turismo': 140, 'cidade': 141, 'cheia': 142, 'gostaria': 143, 'saber': 144, 'vamos': 145, 'conversar': 146, 'maravilhas': 147, 'muitas': 148, 'historias': 149, 'contar': 150, 'delas': 151, 'quer': 152, 'ouvir': 153, 'segundo': 154, 'localizado': 155, 'norte': 156, 'sua': 157, 'capital': 158, 'conhecida': 159, 'metropole': 160, 'fascinante': 161, 'area': 162, '12': 163, 'milhao': 164, 'km2': 165, 'equatorial': 166, 'vastos': 167, 'incrivel': 168, '1616': 169, 'teve': 170, 'periodos': 171, 'mistura': 172, 'tradicoes': 173, 'indigenas': 174, 'africanas': 175, 'portuguesas': 176, 'destaque': 177, 'amazonicas': 178, 'oferece': 179, 'incriveis': 180, 'deve': 181, 'roteiro': 182, 'essencial': 183, 'famoso': 184, '1625': 185, 'peixe': 186, 'carne': 187, 'feira': 188, 'dos': 189, 'belos': 190, 'construido': 191, '1878': 192, 'durante': 193, 'estilo': 194, 'neoclassico': 195, 'elementos': 196, 'amazonicos': 197, 'parque': 198, 'ecologico': 199, '40000': 200, 'm2': 201, 'margens': 202, 'rio': 203, 'guama': 204, 'viveiro': 205, 'borboletas': 206, 'passaros': 207, 'antigo': 208, 'porto': 209, 'revitalizado': 210, '2000': 211, 'restaurantes': 212, 'espaco': 213, 'folclore': 214, 'riquissimo': 215, 'destacamse': 216, 'iara': 217, 'danca': 218, 'origem': 219, 'afroindigena': 220, 'reconhecida': 221, 'patrimonio': 222, 'brasileiro': 223, 'lenda': 224, 'anao': 225, 'cabelos': 226, 'vermelhos': 227, 'pes': 228, 'virados': 229, 'protege': 230, 'festa': 231, 'folclorica': 232, 'conta': 233, 'pai': 234, 'francisco': 235, 'mae': 236, 'catirina': 237, 'morte': 238, 'ressurreicao': 239, 'boi': 240, 'culinaria': 241, 'explosao': 242, 'sabores': 243, 'pratos': 244, 'imperdiveis': 245, 'vatapa': 246, 'prato': 247, 'emblematico': 248, 'assado': 249, 'cozido': 250, 'liquido': 251, 'mandioca': 252, 'brava': 253, 'sopa': 254, 'rua': 255, 'goma': 256, 'seco': 257, 'causa': 258, 'formigamento': 259, 'lingua': 260, 'considerada': 261, 'feijoada': 262, 'feita': 263, 'folhas': 264, 'maniva': 265, 'cozidas': 266, 'por': 267, '7': 268, 'dias': 269, 'carnes': 270, 'arraial': 271, 'pavulagem': 272, 'acontece': 273, '2o': 274, 'domingo': 275, 'reunindo': 276, 'milhoes': 277, 'pessoas': 278, 'procissao': 279, 'catolica': 280, 'ocorre': 281, 'mes': 282, 'aniversario': 283, 'mestre': 284, 'verequete': 285, 'apresentacoes': 286, 'ritmo': 287, 'aereo': 288, 'valdecans': 289, 'terminal': 290, 'fluvial': 291, 'barcos': 292, 'belemjulio': 293, 'cezar': 294, 'ribeiro': 295, 'codigo': 296, 'bel': 297, 'fica': 298, '10': 299, 'km': 300, 'recebendo': 301, 'voos': 302, 'todo': 303, 'recomendo': 304, 'batista': 305, 'campos': 306, 'ou': 307, 'reserve': 308, 'antecedencia': 309, 'periodo': 310, 'melhor': 311, 'epoca': 312, 'mas': 313, 'temperaturas': 314, 'agradaveis': 315, 'fundada': 316, '12011616': 317, 'defender': 318, '18791912': 319, 'trouxe': 320, 'modernizada': 321, 'teatros': 322, 'palacetes': 323, 'sendo': 324, 'chamada': 325, 'paris': 326, 'namerica': 327, '18351840': 328, 'revolta': 329, 'popular': 330, 'graopara': 331, 'contra': 332, 'pobreza': 333, 'cerca': 334, '30': 335, 'mil': 336, 'mortos': 337, 'simbolo': 338, 'resistencia': 339, 'abriga': 340, 'parte': 341, 'tropical': 342, 'unica': 343, 'oncapintada': 344, 'araraazul': 345, 'botocorderosa': 346, 'amazonas': 347, 'tocantins': 348, 'xingu': 349, 'tapajos': 350, 'jari': 351, 'hidrovia': 352, 'vital': 353, 'transporte': 354, 'fauna': 355, 'oncas': 356, 'antas': 357, 'macacos': 358, 'botos': 359, 'araras': 360, 'tucanos': 361, 've': 362, 'destes': 363, 'quente': 364, 'umido': 365, 'media': 366, '26c': 367, 'intensas': 368}\n",
      "Vocabulary size: 369\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer setup\n",
    "# tokenizer = Tokenizer(filters='', oov_token='<OOV>')\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(pd.concat([df['input_clean'], df['response_clean']]))\n",
    "\n",
    "print(tokenizer.word_index)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import random\n",
    "\n",
    "def extract_text_from_pdfs(pdf_folder):\n",
    "    all_text = ''\n",
    "    for filename in os.listdir(pdf_folder):\n",
    "        if filename.endswith('.pdf'):\n",
    "            path = os.path.join(pdf_folder, filename)\n",
    "            with open(path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                for page in reader.pages:\n",
    "                    all_text += page.extract_text() + '\\n'\n",
    "    return all_text\n",
    "\n",
    "pdf_folder = './pdfs'  # altere para o caminho real\n",
    "corpus_text = extract_text_from_pdfs(pdf_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    # text = re.sub(r'\\n+', ' ', text)\n",
    "    # # MANTÉM . ! ? para detecção de fim de frase\n",
    "    # text = re.sub(r'[^a-záéíóúàãõç\\.\\!\\?\\s]', '', text)\n",
    "    # return text\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "    text = re.sub(r'\\n+', '\\n', text)  # mantém quebras de parágrafo\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    text = re.sub(r' +\\n', '\\n', text)\n",
    "    return text.strip()\n",
    "\n",
    "# corpus_text = \"Belém é a capital do Pará. É conhecida pelo Círio de Nazaré! Você já visitou?\"\n",
    "processed_text = preprocess_text(corpus_text)\n",
    "# print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text = processed_text.lower().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus_text)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "imput_sequences = []\n",
    "\n",
    "for line in corpus_text:\n",
    "    tokens = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(tokens)):\n",
    "        n_gram_sequence = tokens[:i+1]\n",
    "        imput_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_length = max(len(seq) for seq in imput_sequences)\n",
    "imput_padded = pad_sequences(imput_sequences, maxlen=max_length, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = imput_padded[:, :-1]\n",
    "ys = imput_padded[:, -1]\n",
    "\n",
    "ys = to_categorical(ys, num_classes=vocab_size)\n",
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # Convert texts to sequences\n",
    "# input_sequences = tokenizer.texts_to_sequences(df['input_clean'])\n",
    "# response_sequences = tokenizer.texts_to_sequences(df['response_clean'])\n",
    "# # \n",
    "# # Pad sequences\n",
    "# max_length = max(max(len(seq) for seq in input_sequences), max(len(seq) for seq in response_sequences))\n",
    "# input_padded = pad_sequences(input_sequences, maxlen=max_length, padding='post')\n",
    "# response_padded = pad_sequences(response_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# # Split data\n",
    "# X_train, X_val, y_train, y_val = train_test_split(input_padded, response_padded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prinako/External/School/UFPA/ITEC_UFPA/24.4_SEMESTER/INTELIGENCIA_COMPUTACIONAL/T2/amazon-portuguese-chatbot/venv/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "I0000 00:00:1743488727.254242  696276 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 349 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Improved Model Architecture\n",
    "embedding_dim = 128  # Reduced from 256 to prevent overfitting\n",
    "lstm_units = 256     # Reduced from 512\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = vocab_size, output_dim = embedding_dim, input_length = max_length-1))\n",
    "model.add(Bidirectional(LSTM(lstm_units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(lstm_units)))\n",
    "model.add(Dense(vocab_size, activation = 'softmax'))\n",
    "\n",
    "# model = Sequential([\n",
    "#     Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "#     BatchNormalization(),  # Helps stabilize training\n",
    "#     Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.2)),\n",
    "#     Bidirectional(LSTM(lstm_units, return_sequences=True)),\n",
    "#     # Dropout(0.3),  # Randomly disable 30% of neurons to prevent over-reliance\n",
    "#     BatchNormalization(),\n",
    "#     Bidirectional(LSTM(lstm_units, return_sequences=True)),\n",
    "#     Bidirectional(LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.2)),\n",
    "#     # Dropout(0.3),\n",
    "#     BatchNormalization(),\n",
    "#     Dense(vocab_size, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# 2. Improved Training Configuration\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 03:29:06.673070: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:501] Allocator (GPU_0_bfc) ran out of memory trying to allocate 148.19MiB (rounded to 155384320)requested by op _EagerConst\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2025-04-01 03:29:06.673104: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1058] BFCAllocator dump for GPU_0_bfc\n",
      "2025-04-01 03:29:06.673117: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (256): \tTotal Chunks: 11, Chunks in use: 11. 2.8KiB allocated for chunks. 2.8KiB in use in bin. 108B client-requested in use in bin.\n",
      "2025-04-01 03:29:06.673125: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (512): \tTotal Chunks: 1, Chunks in use: 0. 768B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-01 03:29:06.673132: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (1024): \tTotal Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.\n",
      "2025-04-01 03:29:06.673136: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (2048): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-01 03:29:06.673140: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (4096): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-01 03:29:06.673143: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (8192): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-01 03:29:06.673147: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (16384): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-01 03:29:06.673150: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (32768): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-01 03:29:06.673153: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (65536): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-01 03:29:06.673157: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-01 03:29:06.673160: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (262144): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-01 03:29:06.673164: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-01 03:29:06.673167: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (1048576): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-01 03:29:06.673170: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-01 03:29:06.673174: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-01 03:29:06.673177: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-01 03:29:06.673181: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-01 03:29:06.673184: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-01 03:29:06.673187: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-01 03:29:06.673195: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (134217728): \tTotal Chunks: 2, Chunks in use: 2. 349.93MiB allocated for chunks. 349.93MiB in use in bin. 296.37MiB client-requested in use in bin.\n",
      "2025-04-01 03:29:06.673199: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1065] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2025-04-01 03:29:06.673203: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1081] Bin for 148.19MiB was 128.00MiB, Chunk State: \n",
      "2025-04-01 03:29:06.673207: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1094] Next region of size 366936064\n",
      "2025-04-01 03:29:06.673215: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 787416000000 of size 1280 next 1\n",
      "2025-04-01 03:29:06.673218: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 787416000500 of size 256 next 2\n",
      "2025-04-01 03:29:06.673220: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 787416000600 of size 256 next 3\n",
      "2025-04-01 03:29:06.673223: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 787416000700 of size 256 next 4\n",
      "2025-04-01 03:29:06.673226: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 787416000800 of size 256 next 5\n",
      "2025-04-01 03:29:06.673228: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 787416000900 of size 256 next 6\n",
      "2025-04-01 03:29:06.673231: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 787416000a00 of size 256 next 7\n",
      "2025-04-01 03:29:06.673234: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 787416000b00 of size 256 next 8\n",
      "2025-04-01 03:29:06.673236: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 787416000c00 of size 256 next 9\n",
      "2025-04-01 03:29:06.673239: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 787416000d00 of size 256 next 10\n",
      "2025-04-01 03:29:06.673242: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 787416000e00 of size 256 next 11\n",
      "2025-04-01 03:29:06.673244: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 787416000f00 of size 256 next 16\n",
      "2025-04-01 03:29:06.673247: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] Free  at 787416001000 of size 768 next 15\n",
      "2025-04-01 03:29:06.673250: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 787416001300 of size 155384320 next 12\n",
      "2025-04-01 03:29:06.673253: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1114] InUse at 78741f430d00 of size 211546880 next 18446744073709551615\n",
      "2025-04-01 03:29:06.673256: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1119]      Summary of in-use Chunks by size: \n",
      "2025-04-01 03:29:06.673261: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1122] 11 Chunks of size 256 totalling 2.8KiB\n",
      "2025-04-01 03:29:06.673265: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1122] 1 Chunks of size 1280 totalling 1.2KiB\n",
      "2025-04-01 03:29:06.673268: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1122] 1 Chunks of size 155384320 totalling 148.19MiB\n",
      "2025-04-01 03:29:06.673272: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1122] 1 Chunks of size 211546880 totalling 201.75MiB\n",
      "2025-04-01 03:29:06.673276: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1126] Sum Total of in-use chunks: 349.94MiB\n",
      "2025-04-01 03:29:06.673279: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1128] Total bytes in pool: 366936064 memory_limit_: 366936064 available bytes: 0 curr_region_allocation_bytes_: 733872128\n",
      "2025-04-01 03:29:06.673286: I external/local_xla/xla/tsl/framework/bfc_allocator.cc:1133] Stats: \n",
      "Limit:                       366936064\n",
      "InUse:                       366935296\n",
      "MaxInUse:                    366935296\n",
      "NumAllocs:                          26\n",
      "MaxAllocSize:                211546880\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2025-04-01 03:29:06.673292: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:512] *************************************************************************************xxxxxxxxxxxxxxx\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInternalError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# # Prepare targets by shifting response sequences\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# y_train_shifted = np.zeros_like(y_train)\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# y_train_shifted[:, :-1] = y_train[:, 1:]\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m \n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# 5. Training with both callbacks\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# X_train, y_train_shifted,\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# validation_data=(X_val, y_val_shifted),\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# callbacks=[early_stopping, reduce_lr],\u001b[39;49;00m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     32\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n\u001b[32m     35\u001b[39m plt.figure(figsize=(\u001b[32m12\u001b[39m, \u001b[32m4\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/External/School/UFPA/ITEC_UFPA/24.4_SEMESTER/INTELIGENCIA_COMPUTACIONAL/T2/amazon-portuguese-chatbot/venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/External/School/UFPA/ITEC_UFPA/24.4_SEMESTER/INTELIGENCIA_COMPUTACIONAL/T2/amazon-portuguese-chatbot/venv/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[39m, in \u001b[36mconvert_to_eager_tensor\u001b[39m\u001b[34m(value, ctx, dtype)\u001b[39m\n\u001b[32m    106\u001b[39m     dtype = dtypes.as_dtype(dtype).as_datatype_enum\n\u001b[32m    107\u001b[39m ctx.ensure_initialized()\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mInternalError\u001b[39m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "# # Prepare targets by shifting response sequences\n",
    "# y_train_shifted = np.zeros_like(y_train)\n",
    "# y_train_shifted[:, :-1] = y_train[:, 1:]\n",
    "# y_val_shifted = np.zeros_like(y_val)\n",
    "# y_val_shifted[:, :-1] = y_val[:, 1:]\n",
    "\n",
    "# # 3. Enhanced Early Stopping\n",
    "# early_stopping = EarlyStopping(\n",
    "#     monitor='val_loss',\n",
    "#     patience=10,           # Increased patience\n",
    "#     min_delta=0.001,       # Minimum change to qualify as improvement\n",
    "#     restore_best_weights=True\n",
    "# )\n",
    "\n",
    "# # 4. Learning Rate Scheduler\n",
    "# reduce_lr = ReduceLROnPlateau(\n",
    "#     monitor='val_loss',\n",
    "#     factor=0.5,\n",
    "#     patience=5,\n",
    "#     min_lr=1e-5\n",
    "# )\n",
    "\n",
    "# 5. Training with both callbacks\n",
    "history = model.fit(\n",
    "    xs, ys,\n",
    "    # X_train, y_train_shifted,\n",
    "    # validation_data=(X_val, y_val_shifted),\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    # callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "# plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy over epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "# plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss over epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save('portuguese_chatbot_vl.h5')\n",
    "\n",
    "# Save the tokenizer\n",
    "import pickle\n",
    "with open('tokenizer_vl.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(input_text, temperature=0.5):\n",
    "    # Preprocess input\n",
    "    cleaned_input = preprocess_text(input_text)\n",
    "    input_seq = tokenizer.texts_to_sequences([cleaned_input])\n",
    "    input_padded = pad_sequences(input_seq, maxlen=max_length, padding='post')\n",
    "    \n",
    "    # Initialize response\n",
    "    response_seq = np.zeros((1, max_length))\n",
    "    response_seq[0, 0] = tokenizer.word_index['<OOV>']  # Start token\n",
    "    \n",
    "    for i in range(1, max_length):\n",
    "        # Predict next word\n",
    "        predictions = model.predict([input_padded, response_seq], verbose=0)[0][i-1]\n",
    "        \n",
    "        # Apply temperature for diversity\n",
    "        predictions = np.log(predictions) / temperature\n",
    "        exp_preds = np.exp(predictions)\n",
    "        predictions = exp_preds / np.sum(exp_preds)\n",
    "        \n",
    "        # Sample from predictions\n",
    "        next_word_idx = np.random.choice(len(predictions), p=predictions)\n",
    "        response_seq[0, i] = next_word_idx\n",
    "        \n",
    "        # Stop if end token is predicted\n",
    "        if next_word_idx == 0:\n",
    "            break\n",
    "    \n",
    "    # Convert sequence to text\n",
    "    response_tokens = [tokenizer.index_word.get(idx, '') for idx in response_seq[0] if idx != 0]\n",
    "    return ' '.join(response_tokens)\n",
    "\n",
    "# Test the chatbot\n",
    "test_inputs = [\n",
    "    \"oi\",\n",
    "    \"como é o clima no pará?\"\n",
    "]\n",
    "\n",
    "for input_text in test_inputs:\n",
    "    response = generate_response(input_text)\n",
    "    print(f\"User: {input_text}\")\n",
    "    print(f\"Chatbot: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(input_text, temperature=0.8):\n",
    "    \"\"\"\n",
    "    Generate a response to the input text using the trained model.\n",
    "    \n",
    "    Args:\n",
    "        input_text: User input string\n",
    "        temperature: Controls randomness of predictions (higher = more random)\n",
    "        \n",
    "    Returns:\n",
    "        Generated response string\n",
    "    \"\"\"\n",
    "    # Preprocess input\n",
    "    cleaned_input = preprocess_text(input_text)\n",
    "    input_seq = tokenizer.texts_to_sequences([cleaned_input])\n",
    "    input_padded = pad_sequences(input_seq, maxlen=max_length, padding='post')\n",
    "    \n",
    "    # Initialize response\n",
    "    response_seq = np.zeros((1, max_length))\n",
    "    response_seq[0, 0] = tokenizer.word_index['<OOV>']  # Start token\n",
    "    \n",
    "    for i in range(1, max_length):\n",
    "        # Predict next word\n",
    "        predictions = model.predict([input_padded, response_seq], verbose=0)[0][i-1]\n",
    "        \n",
    "        # Apply temperature for diversity\n",
    "        predictions = np.log(predictions) / temperature\n",
    "        exp_preds = np.exp(predictions)\n",
    "        predictions = exp_preds / np.sum(exp_preds)\n",
    "        \n",
    "        # Sample from predictions\n",
    "        next_word_idx = np.random.choice(len(predictions), p=predictions)\n",
    "        response_seq[0, i] = next_word_idx\n",
    "        \n",
    "        # Stop if end token is predicted\n",
    "        if next_word_idx == 0:\n",
    "            break\n",
    "    \n",
    "    # Convert sequence to text\n",
    "    response_tokens = []\n",
    "    for idx in response_seq[0]:\n",
    "        if idx == 0:  # Skip padding\n",
    "            continue\n",
    "        word = tokenizer.index_word.get(idx, '')\n",
    "        if word == '<OOV>':  # Skip OOV tokens\n",
    "            continue\n",
    "        response_tokens.append(word)\n",
    "    \n",
    "    return ' '.join(response_tokens)\n",
    "\n",
    "# Test the chatbot\n",
    "test_inputs = [\n",
    "    \"oi\",\n",
    "    \"conte-me sobre o folclore\",\n",
    "    \"quais frutas tem na amazônia?\",\n",
    "    \"quem é o curupira?\",\n",
    "    \"o que é Ver-o-Peso\"\n",
    "]\n",
    "\n",
    "for input_text in test_inputs:\n",
    "    response = generate_response(input_text)\n",
    "    print(f\"User: {input_text}\")\n",
    "    print(f\"Chatbot: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download NLTK punkt data (only needs to be done once)\n",
    "nltk.download('punkt', download_dir='./nltk_data', quiet=True)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Enhanced text preprocessing with NLTK support\"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "    text = re.sub(r'\\n+', '\\n', text)  # preserve paragraph breaks\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)  # collapse multiple spaces\n",
    "    text = re.sub(r' +\\n', '\\n', text)  # remove spaces before newlines\n",
    "    text = re.sub(r'[^\\w\\s\\.\\!\\?]', '', text)  # keep basic punctuation\n",
    "    return text.strip()\n",
    "\n",
    "# Load and preprocess dataset\n",
    "df = pd.read_csv('para_tourism_dataset.csv')\n",
    "\n",
    "# Apply enhanced preprocessing\n",
    "df['input_clean'] = df['input'].apply(preprocess_text)\n",
    "df['response_clean'] = df['response'].apply(preprocess_text)\n",
    "\n",
    "# Tokenization with NLTK sentence-aware processing\n",
    "def tokenize_with_sentences(text):\n",
    "    sentences = sent_tokenize(text, language='portuguese')\n",
    "    return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "# Example usage of sentence tokenization (optional for your use case)\n",
    "# corpus_text = \"Belém é a capital do Pará. É conhecida pelo Círio de Nazaré! Você já visitou?\"\n",
    "# processed_text = preprocess_text(corpus_text)\n",
    "# clean_sentences = tokenize_with_sentences(processed_text)\n",
    "\n",
    "# Main tokenizer for sequences\n",
    "tokenizer = Tokenizer(filters='', oov_token='<OOV>')\n",
    "all_text = pd.concat([df['input_clean'], df['response_clean']]).tolist()\n",
    "tokenizer.fit_on_texts(all_text)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Sequence preparation\n",
    "input_sequences = tokenizer.texts_to_sequences(df['input_clean'])\n",
    "response_sequences = tokenizer.texts_to_sequences(df['response_clean'])\n",
    "\n",
    "max_length = max(max(len(seq) for seq in input_sequences), \n",
    "                max(len(seq) for seq in response_sequences))\n",
    "\n",
    "input_padded = pad_sequences(input_sequences, maxlen=max_length, padding='post')\n",
    "response_padded = pad_sequences(response_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Rest of the model implementation remains the same as previous...\n",
    "# [Include all the model architecture, training, and generation code from earlier]\n",
    "\n",
    "# Enhanced response generation with sentence-aware processing\n",
    "def generate_response_with_sentences(input_text, temperature=0.7):\n",
    "    cleaned_input = preprocess_text(input_text)\n",
    "    input_seq = tokenizer.texts_to_sequences([cleaned_input])\n",
    "    input_padded = pad_sequences(input_seq, maxlen=max_length, padding='post')\n",
    "    \n",
    "    response_seq = np.zeros((1, max_length))\n",
    "    response_seq[0, 0] = tokenizer.word_index['<OOV>']\n",
    "    \n",
    "    for i in range(1, max_length):\n",
    "        predictions = model.predict([input_padded, response_seq], verbose=0)[0][i-1]\n",
    "        predictions = np.log(predictions) / temperature\n",
    "        exp_preds = np.exp(predictions)\n",
    "        predictions = exp_preds / np.sum(exp_preds)\n",
    "        \n",
    "        next_word_idx = np.random.choice(len(predictions), p=predictions)\n",
    "        response_seq[0, i] = next_word_idx\n",
    "        \n",
    "        if next_word_idx == 0:\n",
    "            break\n",
    "    \n",
    "    response_tokens = [tokenizer.index_word.get(idx, '') for idx in response_seq[0] if idx != 0]\n",
    "    raw_response = ' '.join(response_tokens)\n",
    "    \n",
    "    # Post-process with sentence tokenization for better formatting\n",
    "    sentences = sent_tokenize(raw_response, language='portuguese')\n",
    "    return ' '.join(s.strip() for s in sentences if s.strip())\n",
    "\n",
    "# Test the enhanced version\n",
    "test_questions = [\n",
    "    \"oi\",\n",
    "    \"quais os melhores pontos turísticos de Belém?\",\n",
    "    \"conte-me sobre o carimbó\",\n",
    "    \"o que é pato no tucupi?\"\n",
    "]\n",
    "\n",
    "print(\"Enhanced Chatbot with NLTK Processing:\")\n",
    "for question in test_questions:\n",
    "    print(f\"Usuário: {question}\")\n",
    "    print(f\"Chatbot: {generate_response_with_sentences(question)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
